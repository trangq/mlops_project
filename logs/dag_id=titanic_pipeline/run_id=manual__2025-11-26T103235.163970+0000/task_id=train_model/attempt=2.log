[2025-11-26T10:52:28.833+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2025-11-26T10:52:28.920+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: titanic_pipeline.train_model manual__2025-11-26T10:32:35.163970+00:00 [queued]>
[2025-11-26T10:52:28.936+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: titanic_pipeline.train_model manual__2025-11-26T10:32:35.163970+00:00 [queued]>
[2025-11-26T10:52:28.936+0000] {taskinstance.py:2306} INFO - Starting attempt 2 of 2
[2025-11-26T10:52:28.957+0000] {taskinstance.py:2330} INFO - Executing <Task(BashOperator): train_model> on 2025-11-26 10:32:35.163970+00:00
[2025-11-26T10:52:28.963+0000] {standard_task_runner.py:63} INFO - Started process 566 to run task
[2025-11-26T10:52:28.968+0000] {standard_task_runner.py:90} INFO - Running: ['***', 'tasks', 'run', 'titanic_pipeline', 'train_model', 'manual__2025-11-26T10:32:35.163970+00:00', '--job-id', '34', '--raw', '--subdir', 'DAGS_FOLDER/training_pipeline_dag.py', '--cfg-path', '/tmp/tmpks23olj0']
[2025-11-26T10:52:28.971+0000] {standard_task_runner.py:91} INFO - Job 34: Subtask train_model
[2025-11-26T10:52:28.995+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/***/settings.py:195: DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
  SQL_ALCHEMY_CONN = conf.get("database", "SQL_ALCHEMY_CONN")

[2025-11-26T10:52:29.066+0000] {task_command.py:426} INFO - Running <TaskInstance: titanic_pipeline.train_model manual__2025-11-26T10:32:35.163970+00:00 [running]> on host 86765dc6c081
[2025-11-26T10:52:29.192+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='titanic_pipeline' AIRFLOW_CTX_TASK_ID='train_model' AIRFLOW_CTX_EXECUTION_DATE='2025-11-26T10:32:35.163970+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-11-26T10:32:35.163970+00:00'
[2025-11-26T10:52:29.196+0000] {taskinstance.py:430} INFO - ::endgroup::
[2025-11-26T10:52:29.250+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-11-26T10:52:29.253+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'python /opt/***/scripts/train_model.py /opt/***/data/cleaned_data.csv']
[2025-11-26T10:52:29.269+0000] {subprocess.py:86} INFO - Output:
[2025-11-26T10:52:58.348+0000] {subprocess.py:93} INFO - 2025/11/26 10:52:58 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.
[2025-11-26T10:53:03.039+0000] {subprocess.py:93} INFO - /home/***/.local/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.
[2025-11-26T10:53:03.080+0000] {subprocess.py:93} INFO -   warnings.warn(
[2025-11-26T10:53:03.081+0000] {subprocess.py:93} INFO - /home/***/.local/lib/python3.10/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.
[2025-11-26T10:53:03.083+0000] {subprocess.py:93} INFO -   warnings.warn(
[2025-11-26T10:53:44.248+0000] {job.py:218} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "***-postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 192, in heartbeat
    self._merge_from(Job._fetch_from_db(self, session))
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/api_internal/internal_api_call.py", line 115, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/retries.py", line 89, in wrapped_function
    for attempt in run_with_db_retries(max_retries=retries, logger=logger, **retry_kwargs):
  File "/home/airflow/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 435, in __iter__
    do = self.iter(retry_state=retry_state)
  File "/home/airflow/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 368, in iter
    result = action(retry_state)
  File "/home/airflow/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 410, in exc_check
    raise retry_exc.reraise()
  File "/home/airflow/.local/lib/python3.10/site-packages/tenacity/__init__.py", line 183, in reraise
    raise self.last_attempt.result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 451, in result
    return self.__get_result()
  File "/usr/local/lib/python3.10/concurrent/futures/_base.py", line 403, in __get_result
    raise self._exception
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/retries.py", line 98, in wrapped_function
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 316, in _fetch_from_db
    session.merge(job)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/future/engine.py", line 412, in connect
    return super(Engine, self).connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3325, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3404, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3374, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2208, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3371, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "***-postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2025-11-26T10:53:58.146+0000] {job.py:226} ERROR - Job heartbeat failed with error. Scheduler is in unhealthy state
[2025-11-26T10:55:00.320+0000] {job.py:226} ERROR - Job heartbeat failed with error. Scheduler is in unhealthy state
[2025-11-26T10:55:17.923+0000] {subprocess.py:93} INFO - 2025/11/26 10:55:17 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmps5c11jfy/model/model.pkl, flavor: sklearn). Fall back to return ['scikit-learn==1.7.2', 'cloudpickle==3.1.2']. Set logging level to DEBUG to see the full traceback.
[2025-11-26T10:55:40.220+0000] {job.py:226} ERROR - Job heartbeat failed with error. Scheduler is in unhealthy state
[2025-11-26T10:56:01.276+0000] {subprocess.py:93} INFO - Loading processed data from: /opt/***/data/cleaned_data.csv
[2025-11-26T10:56:01.301+0000] {subprocess.py:93} INFO - 
[2025-11-26T10:56:01.303+0000] {subprocess.py:93} INFO - === Logging model to MLflow ===
[2025-11-26T10:56:01.304+0000] {subprocess.py:93} INFO - üèÉ View run indecisive-croc-269 at: http://mlflow-server:5000/#/experiments/1/runs/ec25632f324549f59dc721fb55a40946
[2025-11-26T10:56:01.307+0000] {subprocess.py:93} INFO - üß™ View experiment at: http://mlflow-server:5000/#/experiments/1
[2025-11-26T10:56:01.310+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-11-26T10:56:01.311+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/boto3/s3/transfer.py", line 371, in upload_file
[2025-11-26T10:56:01.313+0000] {subprocess.py:93} INFO -     future.result()
[2025-11-26T10:56:01.319+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/s3transfer/futures.py", line 103, in result
[2025-11-26T10:56:01.323+0000] {subprocess.py:93} INFO -     return self._coordinator.result()
[2025-11-26T10:56:01.325+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/s3transfer/futures.py", line 266, in result
[2025-11-26T10:56:01.327+0000] {subprocess.py:93} INFO -     raise self._exception
[2025-11-26T10:56:01.332+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/s3transfer/tasks.py", line 139, in __call__
[2025-11-26T10:56:01.334+0000] {subprocess.py:93} INFO -     return self._execute_main(kwargs)
[2025-11-26T10:56:01.340+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/s3transfer/tasks.py", line 162, in _execute_main
[2025-11-26T10:56:01.342+0000] {subprocess.py:93} INFO -     return_value = self._main(**kwargs)
[2025-11-26T10:56:01.344+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/s3transfer/upload.py", line 764, in _main
[2025-11-26T10:56:01.345+0000] {subprocess.py:93} INFO -     client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)
[2025-11-26T10:56:01.346+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/botocore/client.py", line 565, in _api_call
[2025-11-26T10:56:01.347+0000] {subprocess.py:93} INFO -     return self._make_api_call(operation_name, kwargs)
[2025-11-26T10:56:01.348+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/botocore/client.py", line 1021, in _make_api_call
[2025-11-26T10:56:01.350+0000] {subprocess.py:93} INFO -     raise error_class(parsed_response, operation_name)
[2025-11-26T10:56:01.352+0000] {subprocess.py:93} INFO - botocore.errorfactory.NoSuchBucket: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist
[2025-11-26T10:56:01.354+0000] {subprocess.py:93} INFO - 
[2025-11-26T10:56:01.355+0000] {subprocess.py:93} INFO - During handling of the above exception, another exception occurred:
[2025-11-26T10:56:01.357+0000] {subprocess.py:93} INFO - 
[2025-11-26T10:56:01.359+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-11-26T10:56:01.361+0000] {subprocess.py:93} INFO -   File "/opt/***/scripts/train_model.py", line 90, in <module>
[2025-11-26T10:56:01.362+0000] {subprocess.py:93} INFO -     train(sys.argv[1])
[2025-11-26T10:56:01.367+0000] {subprocess.py:93} INFO -   File "/opt/***/scripts/train_model.py", line 57, in train
[2025-11-26T10:56:01.369+0000] {subprocess.py:93} INFO -     model_info = mlflow.sklearn.log_model(
[2025-11-26T10:56:01.370+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/mlflow/sklearn/__init__.py", line 426, in log_model
[2025-11-26T10:56:01.371+0000] {subprocess.py:93} INFO -     return Model.log(
[2025-11-26T10:56:01.379+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/mlflow/models/model.py", line 1297, in log
[2025-11-26T10:56:01.381+0000] {subprocess.py:93} INFO -     client.log_model_artifacts(model.model_id, local_path)
[2025-11-26T10:56:01.382+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/mlflow/tracking/client.py", line 5643, in log_model_artifacts
[2025-11-26T10:56:01.383+0000] {subprocess.py:93} INFO -     return self._tracking_client.log_model_artifacts(model_id, local_dir)
[2025-11-26T10:56:01.385+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/client.py", line 911, in log_model_artifacts
[2025-11-26T10:56:01.386+0000] {subprocess.py:93} INFO -     self._get_artifact_repo(model_id, resource="logged_model").log_artifacts(local_dir)
[2025-11-26T10:56:01.388+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/mlflow/store/artifact/s3_artifact_repo.py", line 299, in log_artifacts
[2025-11-26T10:56:01.390+0000] {subprocess.py:93} INFO -     self._upload_file(
[2025-11-26T10:56:01.395+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/mlflow/store/artifact/s3_artifact_repo.py", line 247, in _upload_file
[2025-11-26T10:56:01.397+0000] {subprocess.py:93} INFO -     s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)
[2025-11-26T10:56:01.398+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/boto3/s3/inject.py", line 145, in upload_file
[2025-11-26T10:56:01.399+0000] {subprocess.py:93} INFO -     return transfer.upload_file(
[2025-11-26T10:56:01.401+0000] {subprocess.py:93} INFO -   File "/home/***/.local/lib/python3.10/site-packages/boto3/s3/transfer.py", line 377, in upload_file
[2025-11-26T10:56:01.403+0000] {subprocess.py:93} INFO -     raise S3UploadFailedError(
[2025-11-26T10:56:01.409+0000] {subprocess.py:93} INFO - boto3.exceptions.S3UploadFailedError: Failed to upload /tmp/tmps5c11jfy/model/python_env.yaml to mlflow-artifacts/1/models/m-4722d67c08624b6eb56891a507c9df81/artifacts/python_env.yaml: An error occurred (NoSuchBucket) when calling the PutObject operation: The specified bucket does not exist
[2025-11-26T10:56:03.710+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-11-26T10:56:03.787+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2025-11-26T10:56:05.241+0000] {taskinstance.py:2905} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 465, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 432, in _execute_callable
    return execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/baseoperator.py", line 401, in wrapper
    return func(self, *args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/operators/bash.py", line 243, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-11-26T10:56:05.428+0000] {taskinstance.py:1206} INFO - Marking task as FAILED. dag_id=titanic_pipeline, task_id=train_model, run_id=manual__2025-11-26T10:32:35.163970+00:00, execution_date=20251126T103235, start_date=20251126T105228, end_date=20251126T105605
[2025-11-26T10:56:05.989+0000] {standard_task_runner.py:110} ERROR - Failed to execute job 34 for task train_model (Bash command failed. The command returned a non-zero exit code 1.; 566)
[2025-11-26T10:56:06.145+0000] {local_task_job_runner.py:240} INFO - Task exited with return code 1
[2025-11-26T10:56:09.057+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-11-26T10:56:09.073+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
